import feedparser
import time
import datetime
import os
import re 
import requests # [‡πÄ‡∏û‡∏¥‡πà‡∏°] ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ requests ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏ö‡∏≠‡∏ó
from config import RSS_SOURCES
from utils import get_timestamp, load_history, save_history, send_discord_webhook
from aya_brain import aya_process_news

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î User-Agent ‡πÉ‡∏´‡πâ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ñ‡∏ô‡πÉ‡∏ä‡πâ Browser ‡∏à‡∏£‡∏¥‡∏á‡πÜ
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
}

def fetch_rss_feed(url):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á RSS ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ requests ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏ö‡∏•‡πá‡∏≠‡∏Å feedparser
    """
    try:
        # 1. ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ requests ‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏ô‡∏µ‡∏¢‡∏ô‡∏Å‡∏ß‡πà‡∏≤)
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        # ‡∏™‡πà‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (content) ‡πÑ‡∏õ‡πÉ‡∏´‡πâ feedparser ‡πÅ‡∏õ‡∏•‡∏á
        feed = feedparser.parse(response.content)
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏´‡∏°
        if feed.bozo == 0 or len(feed.entries) > 0:
            return feed
            
    except Exception as e:
        print(f"   ‚ö†Ô∏è Requests failed ({e}), trying fallback...")

    # 2. ‡∏ñ‡πâ‡∏≤‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ú‡∏• ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ feedparser ‡∏î‡∏∂‡∏á‡∏ï‡∏£‡∏á‡πÜ (Fallback)
    return feedparser.parse(url)

def extract_image(entry):
    """‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á URL ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏Ç‡πà‡∏≤‡∏ß"""
    if 'media_content' in entry:
        try: return entry.media_content[0]['url']
        except: pass
    if 'media_thumbnail' in entry:
        try: return entry.media_thumbnail[0]['url']
        except: pass
    if 'links' in entry:
        for link in entry.links:
            if link.type.startswith('image/'):
                return link.href
    content_html = ""
    if 'content' in entry:
        content_html = entry.content[0].value
    elif 'summary' in entry:
        content_html = entry.summary
    if content_html:
        match = re.search(r'<img [^>]*src="([^"]+)"', content_html)
        if match: return match.group(1)
    return None

def is_interesting_reddit_post(entry):
    """‡∏Å‡∏£‡∏≠‡∏á‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ Reddit"""
    wanted_flairs = ["News", "Game News", "Merchandise", "Cosplay", "Official News", "Game Discussion"]
    
    if 'tags' in entry:
        for tag in entry.tags:
            if tag.term in wanted_flairs:
                return True
                
    title_lower = entry.title.lower()
    if "[news]" in title_lower or "[game]" in title_lower or "release" in title_lower:
        return True
        
    return False

def run_once():
    print(f"[{datetime.datetime.now()}] üå™Ô∏è ‡∏≠‡∏≤‡∏¢‡∏∞‡∏ï‡∏∑‡πà‡∏ô‡∏°‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ç‡πà‡∏≤‡∏ß‡∏£‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà...")
    
    webhook_env = os.getenv("DISCORD_WEBHOOK_URL")
    target_webhooks = []
    if webhook_env:
        target_webhooks = [url.strip() for url in webhook_env.split(',') if url.strip()]
    
    print(f"üì° ‡∏û‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(target_webhooks)} ‡πÅ‡∏´‡πà‡∏á")

    read_history = load_history()
    new_items_found = False

    processed_count = 0 
    MAX_NEWS_PER_RUN = 10 

    for source in RSS_SOURCES:
        print(f"Flying to... {source['name']} ü¶Ö")
        try:
            # [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà fetch_rss_feed ‡πÅ‡∏ó‡∏ô feedparser.parse ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            feed = fetch_rss_feed(source['url'])
            
            print(f"   üîé ‡πÄ‡∏à‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(feed.entries)} ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô Feed ‡∏ô‡∏µ‡πâ")
            
            if len(feed.entries) == 0:
                print("   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡∏¢ (‡πÄ‡∏ß‡πá‡∏ö‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ö‡∏•‡πá‡∏≠‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ú‡∏¥‡∏î)")
                continue

            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 100 ‡∏Ç‡πà‡∏≤‡∏ß (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤)
            check_limit = 100 
            
            for entry in feed.entries[:check_limit]:
                
                if processed_count >= MAX_NEWS_PER_RUN:
                    print("üõë ‡∏™‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏Ñ‡∏£‡∏ö‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡∏ï‡πà‡∏≠‡∏£‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß ‡∏û‡∏±‡∏Å‡∏Å‡πà‡∏≠‡∏ô...")
                    break

                news_id = entry.id if 'id' in entry else entry.link
                
                # ‡∏Å‡∏£‡∏≠‡∏á Reddit
                if source['type'] == 'community':
                    if not is_interesting_reddit_post(entry):
                        continue 
                
                if news_id not in read_history:
                    print(f"     ‚ú® ‡πÄ‡∏à‡∏≠‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {entry.title}")
                    pub_date = get_timestamp(entry)
                    
                    content = ""
                    if 'content' in entry:
                        content = entry.content[0].value
                    elif 'summary' in entry:
                        content = entry.summary
                    
                    image_url = extract_image(entry)
                    
                    aya_article = aya_process_news(source['type'], entry.title, content, entry.link, pub_date)
                    
                    if "AI_ERROR" in aya_article:
                        print(f"     üí® Error: {aya_article}")
                        if "429" in aya_article: 
                            print("‚õî ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡πÄ‡∏ï‡πá‡∏° (429) ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ")
                            processed_count = MAX_NEWS_PER_RUN
                            break
                    elif "SKIP" in aya_article:
                        print("     üóëÔ∏è (‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡πà‡∏≤‡πÄ‡∏ö‡∏∑‡πà‡∏≠ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ)")
                        read_history.append(news_id)
                        new_items_found = True
                        time.sleep(2) 
                    else:
                        print("\n" + "üì∞"*20)
                        print(f"üìç {source['name']} | üïí {pub_date}")
                        print(aya_article)
                        if image_url: print(f"üñºÔ∏è Image: {image_url}")
                        print("-" * 50)
                        
                        if target_webhooks:
                            for i, webhook_url in enumerate(target_webhooks):
                                print(f"üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á (Embed) ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà {i+1}...")
                                send_discord_webhook(
                                    webhook_url, 
                                    aya_article, 
                                    source['name'], 
                                    news_url=entry.link, 
                                    image_url=image_url,
                                    pub_date=pub_date
                                )
                        else:
                            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ DISCORD_WEBHOOK_URL")
                        
                        read_history.append(news_id)
                        new_items_found = True
                        processed_count += 1
                        
                        print("‚è≥ ‡∏û‡∏±‡∏Å 20 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ...")
                        time.sleep(20) 

        except Exception as e:
            print(f"‚ö†Ô∏è Error accessing {source['name']}: {e}")

    if new_items_found:
        save_history(read_history)
        print("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    else:
        print("üí§ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà")

if __name__ == "__main__":
    run_once()
