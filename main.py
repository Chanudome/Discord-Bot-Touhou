import feedparser
import time
import datetime
import os
import re 
from config import RSS_SOURCES
from utils import get_timestamp, load_history, save_history, send_discord_webhook
from aya_brain import aya_process_news

# [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î User-Agent ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡πá‡∏ö‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏ô‡∏∂‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡∏π‡∏ú‡πà‡∏≤‡∏ô Browser (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Found 0 news)
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

def extract_image(entry):
    """‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á URL ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏Ç‡πà‡∏≤‡∏ß"""
    if 'media_content' in entry:
        try: return entry.media_content[0]['url']
        except: pass
    if 'media_thumbnail' in entry:
        try: return entry.media_thumbnail[0]['url']
        except: pass
    if 'links' in entry:
        for link in entry.links:
            if link.type.startswith('image/'):
                return link.href
    content_html = ""
    if 'content' in entry:
        content_html = entry.content[0].value
    elif 'summary' in entry:
        content_html = entry.summary
    if content_html:
        match = re.search(r'<img [^>]*src="([^"]+)"', content_html)
        if match: return match.group(1)
    return None

def is_interesting_reddit_post(entry):
    """
    ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ Reddit ‡∏ô‡∏µ‡πâ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡πÑ‡∏´‡∏° (‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Flair ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
    """
    wanted_flairs = ["News", "Game News", "Merchandise", "Cosplay", "Official News", "Game Discussion"]
    
    if 'tags' in entry:
        for tag in entry.tags:
            if tag.term in wanted_flairs:
                return True
                
    title_lower = entry.title.lower()
    if "[news]" in title_lower or "[game]" in title_lower or "release" in title_lower:
        return True
        
    return False

def run_once():
    print(f"[{datetime.datetime.now()}] üå™Ô∏è ‡∏≠‡∏≤‡∏¢‡∏∞‡∏ï‡∏∑‡πà‡∏ô‡∏°‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ç‡πà‡∏≤‡∏ß‡∏£‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà...")
    
    webhook_env = os.getenv("DISCORD_WEBHOOK_URL")
    target_webhooks = []
    if webhook_env:
        target_webhooks = [url.strip() for url in webhook_env.split(',') if url.strip()]
    
    print(f"üì° ‡∏û‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(target_webhooks)} ‡πÅ‡∏´‡πà‡∏á")

    read_history = load_history()
    new_items_found = False

    processed_count = 0 
    MAX_NEWS_PER_RUN = 10 

    for source in RSS_SOURCES:
        print(f"Flying to... {source['name']} ü¶Ö")
        try:
            # [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡πÉ‡∏™‡πà agent=USER_AGENT ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏ß‡πá‡∏ö Yomoyama/Garakuta ‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏ö‡∏≠‡∏ó
            feed = feedparser.parse(source['url'], agent=USER_AGENT)
            
            print(f"   üîé ‡πÄ‡∏à‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(feed.entries)} ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô Feed ‡∏ô‡∏µ‡πâ")
            
            if len(feed.entries) == 0:
                print("   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡∏¢ (‡πÄ‡∏ß‡πá‡∏ö‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ö‡∏•‡πá‡∏≠‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ú‡∏¥‡∏î)")
                continue

            # [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ä‡πá‡∏Ñ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô 100 (‡πÄ‡∏ú‡∏∑‡πà‡∏≠ Reddit Fanart ‡∏ñ‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏ô‡∏°‡∏¥‡∏î)
            check_limit = 100 
            
            for entry in feed.entries[:check_limit]:
                
                if processed_count >= MAX_NEWS_PER_RUN:
                    print("üõë ‡∏™‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏Ñ‡∏£‡∏ö‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡∏ï‡πà‡∏≠‡∏£‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß ‡∏û‡∏±‡∏Å‡∏Å‡πà‡∏≠‡∏ô...")
                    break

                news_id = entry.id if 'id' in entry else entry.link
                
                # ‡∏Å‡∏£‡∏≠‡∏á Reddit
                if source['type'] == 'community':
                    if not is_interesting_reddit_post(entry):
                        continue # ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ
                
                if news_id not in read_history:
                    print(f"     ‚ú® ‡πÄ‡∏à‡∏≠‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {entry.title}")
                    pub_date = get_timestamp(entry)
                    
                    content = ""
                    if 'content' in entry:
                        content = entry.content[0].value
                    elif 'summary' in entry:
                        content = entry.summary
                    
                    image_url = extract_image(entry)
                    
                    aya_article = aya_process_news(source['type'], entry.title, content, entry.link, pub_date)
                    
                    if "AI_ERROR" in aya_article:
                        print(f"     üí® Error: {aya_article}")
                        if "429" in aya_article: 
                            print("‚õî ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡πÄ‡∏ï‡πá‡∏° (429) ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ")
                            processed_count = MAX_NEWS_PER_RUN
                            break
                    elif "SKIP" in aya_article:
                        print("     üóëÔ∏è (‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡πà‡∏≤‡πÄ‡∏ö‡∏∑‡πà‡∏≠ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ)")
                        read_history.append(news_id)
                        new_items_found = True
                        time.sleep(2) 
                    else:
                        print("\n" + "üì∞"*20)
                        print(f"üìç {source['name']} | üïí {pub_date}")
                        print(aya_article)
                        if image_url: print(f"üñºÔ∏è Image: {image_url}")
                        print("-" * 50)
                        
                        if target_webhooks:
                            for i, webhook_url in enumerate(target_webhooks):
                                print(f"üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á (Embed) ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà {i+1}...")
                                send_discord_webhook(
                                    webhook_url, 
                                    aya_article, 
                                    source['name'], 
                                    news_url=entry.link, 
                                    image_url=image_url,
                                    pub_date=pub_date
                                )
                        else:
                            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ DISCORD_WEBHOOK_URL")
                        
                        read_history.append(news_id)
                        new_items_found = True
                        processed_count += 1
                        
                        print("‚è≥ ‡∏û‡∏±‡∏Å 20 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ...")
                        time.sleep(20) 

        except Exception as e:
            print(f"‚ö†Ô∏è Error accessing {source['name']}: {e}")

    if new_items_found:
        save_history(read_history)
        print("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    else:
        print("üí§ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà")

if __name__ == "__main__":
    run_once()
